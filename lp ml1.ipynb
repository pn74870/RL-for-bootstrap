{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jun  7 16:17:46 2024\n",
    "\n",
    "@author: User\n",
    "\n",
    "1a: There is only 1 value in each alphas.\n",
    "\"\"\"\n",
    "\n",
    "from CFT_lego import CFT_lego\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from DG_analytic_v2 import DG\n",
    "from Where_execute import Where_execute\n",
    "\n",
    "root_dir = Where_execute()\n",
    "cft = CFT_lego()\n",
    "\n",
    "class LP:\n",
    "    def __init__(self, low_delta_list, delta_truncation, delta_spacing, delta_max, \n",
    "                 rescale_factor, how_many_constraints,data):\n",
    "        self.low_delta_list = low_delta_list\n",
    "        self.delta_truncation = delta_truncation\n",
    "        self.delta_spacing = delta_spacing\n",
    "        self.delta_max = delta_max\n",
    "        self.rescale_factor = rescale_factor\n",
    "        \n",
    "        self.num_grid = int((delta_max-max(low_delta_list))/ delta_spacing)\n",
    "        self.delta_list = low_delta_list+[delta_truncation+delta_spacing*i for i in range(1, self.num_grid+2)]\n",
    "        \n",
    "        dn_constraints = [2*i+1 for i in range(how_many_constraints)]\n",
    "        \n",
    "        self.derivative_constraints = DG(data, self.delta_list, 2, how_many_constraints)*np.exp(-rescale_factor*np.array(self.delta_list))\n",
    "        self.solver = np.array([2]+[0 for _ in range(how_many_constraints-1)])\n",
    "        \n",
    "        self.a1_eq = np.array(list(DG(data, self.low_delta_list, 2, 1)[0]*np.exp(-rescale_factor*np.array(self.low_delta_list))) + \n",
    "                              [0 for _ in range(1, self.num_grid+2)])\n",
    "    \n",
    "    def a1_bound(self):\n",
    "        # Define and solve the CVXPY problem.\n",
    "        ope = cp.Variable(len(self.delta_list))\n",
    "        prob_low = cp.Problem(cp.Minimize(self.a1_eq@ope-2),\n",
    "                               [self.derivative_constraints @ ope == self.solver, \n",
    "                                # ope[:len(self.low_delta_list)]>=1e-5,\n",
    "                                ope>=0])\n",
    "        \n",
    "        prob_up = cp.Problem(cp.Maximize(self.a1_eq@ope-2),\n",
    "                               [self.derivative_constraints @ ope == self.solver, \n",
    "                                # ope[:len(self.low_delta_list)]>=1e-5,\n",
    "                                ope>=0])\n",
    "        \n",
    "        try:\n",
    "            prob_low.solve()\n",
    "            prob_up.solve()\n",
    "            if type(prob_low.value) == np.float64 and type(prob_up.value) == np.float64:\n",
    "                return [prob_low.value, prob_up.value]\n",
    "            else:\n",
    "               # print('Inf bound: ', [prob_low.value, prob_up.value])\n",
    "                return None\n",
    "        except:\n",
    "           # print('No bound')\n",
    "            return None\n",
    "    \n",
    "    def alpha_criteria(self):\n",
    "        a1_range = self.a1_bound()\n",
    "        if a1_range == None:\n",
    "            return 'No such bound'\n",
    "        else:\n",
    "            return 'Exist bound', a1_range\n",
    "\n",
    "class LP_data:\n",
    "    def __init__(self, low_delta_list, delta_truncation, delta_spacing, delta_max, rescale_factor):\n",
    "        self.low_delta_list = low_delta_list\n",
    "        self.delta_truncation = delta_truncation\n",
    "        self.delta_spacing = delta_spacing\n",
    "        self.delta_max = delta_max\n",
    "        self.rescale_factor = rescale_factor\n",
    "    \n",
    "    def stored_a_bound(self, how_many_constraints):\n",
    "        import os\n",
    "        import json\n",
    "        file_name = f'LP_data/a_region_constraint_num{how_many_constraints}_with_alphabound.json'\n",
    "        \n",
    "        d_property = {'delta':self.low_delta_list, 'd_truncate':self.delta_truncation, \n",
    "                      'd_space':self.delta_spacing, 'd_max':self.delta_max, 'd_rf':self.rescale_factor}\n",
    "        \n",
    "        if os.path.isfile(root_dir+file_name):\n",
    "            with open(os.path.join(root_dir, file_name), 'r') as file:\n",
    "                combined_data = json.load(file)\n",
    "            # 将 target_label 转换为 JSON 字符串\n",
    "            target_label_str = json.dumps(d_property)\n",
    "            \n",
    "            # 获取对应的值\n",
    "            values = combined_data.get(target_label_str)\n",
    "            \n",
    "            if values is None:\n",
    "                print('Building new alpha json element!')\n",
    "                lp = LP(self.low_delta_list, self.delta_truncation, \n",
    "                        self.delta_spacing, self.delta_max, self.rescale_factor, how_many_constraints)\n",
    "                a_bound = lp.alpha_criteria()\n",
    "                alpha_container = {'property':a_bound[0], 'alpha bound': a_bound[1]}\n",
    "                # 创建以 d_property 标签为键的字典，值为 alpha_container\n",
    "                combined_data[json.dumps(d_property)] = alpha_container\n",
    "                \n",
    "                # Write to JSON file\n",
    "                with open(os.path.join(root_dir, file_name), 'w') as file:\n",
    "                    json.dump(combined_data, file, indent=4)\n",
    "                \n",
    "                return alpha_container\n",
    "            else:\n",
    "                return values\n",
    "        else:\n",
    "            print('Building new alpha json file!')\n",
    "            combined_data={}\n",
    "            lp = LP(self.low_delta_list, self.delta_truncation, \n",
    "                    self.delta_spacing, self.delta_max, self.rescale_factor, how_many_constraints)\n",
    "            a_bound = lp.alpha_criteria()\n",
    "            alpha_container = {'property':a_bound[0], 'alpha bound': a_bound[1]}\n",
    "            # 创建以 d_property 标签为键的字典，值为 alpha_container\n",
    "            combined_data[json.dumps(d_property)] = alpha_container\n",
    "            \n",
    "            # Write to JSON file\n",
    "            with open(os.path.join(root_dir, file_name), 'w') as file:\n",
    "                json.dump(combined_data, file, indent=4)\n",
    "            return alpha_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from get_allowed_2states import get_allowed_2states\n",
    "import random\n",
    "\n",
    "# Define the custom environment\n",
    "class LpEnv(gym.Env):\n",
    "    def __init__(self,deltas_1_2,data,max_action=.5,delta_spacing=0.1,delta_max=50,penalty_not_allowed=5.,step_cost=.1,nder=5,max_steps=100,rew=1,delta_tol=.1):\n",
    "        super(LpEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=2, high=100, shape=(5,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-max_action, high=max_action, shape=(1,), dtype=np.float32)\n",
    "        self.delta_spacing=delta_spacing\n",
    "        self.delta_max=delta_max\n",
    "        self.state = np.random.rand(3)\n",
    "        self.penalty_not_allowed=penalty_not_allowed\n",
    "        self.step_cost=step_cost\n",
    "        self.nder=nder\n",
    "        self.deltas_1_2=deltas_1_2\n",
    "        self.max_steps=max_steps\n",
    "        self.current_step=0\n",
    "        self.reward =rew\n",
    "        self.data=data\n",
    "        self.delta_tol=delta_tol\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step+=1\n",
    "        self.state[2]+=action  \n",
    "        if self.state[2]<self.state[1]:\n",
    "            self.state[2]=self.state[1]+(self.state[1]-self.state[2])\n",
    "\n",
    "        bounds1=self.get_lp_bound(self.state.tolist())\n",
    "        bounds2=self.get_lp_bound((self.state+np.array([0,0,self.delta_tol])).tolist())\n",
    "\n",
    "        if bounds1!=None and bounds2==None:\n",
    "            reward=self.reward\n",
    "            done=True\n",
    "            truncated=False\n",
    "        elif bounds1==None:\n",
    "            reward=-self.penalty_not_allowed\n",
    "            done=True\n",
    "            truncated=False\n",
    "        else:\n",
    "            reward=-self.step_cost\n",
    "            truncated = self.current_step>=self.max_steps\n",
    "            done=truncated\n",
    "        bounds1=np.array(bounds1)\n",
    "        if(bounds1.shape!=(2,)): bounds1=np.array([0,-1])\n",
    "        return np.concatenate((self.state,bounds1)), reward, done, truncated,{}\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step=0\n",
    "        [delta1,delta2]=self.random_deltas()\n",
    "        self.state =np.array([delta1,delta2,delta2],dtype=np.float32)\n",
    "        bounds1=self.get_lp_bound(self.state.tolist())\n",
    "        bounds1=np.array(bounds1)\n",
    "        if(bounds1.shape!=(2,)): bounds1=np.array([0,-1])\n",
    "        return np.concatenate((self.state,bounds1),dtype=np.float32),{}\n",
    "    \n",
    "    def random_deltas(self):\n",
    "        return random.choice(self.deltas_1_2)\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def get_lp_bound(self, state):\n",
    "        \n",
    "        lp =LP( state, state[-1], self.delta_spacing, self.delta_max, \n",
    "                 0.8, self.nder,self.data)\n",
    "       \n",
    "        \n",
    "\n",
    "        return lp.a1_bound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_allowed_2states import get_allowed_2states\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "path = root_dir+\"LP_data/isingDGn_v5.csv\"\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "arr=get_allowed_2states()\n",
    "filtered_data = [item for item in arr if item[1] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if done := self.locals[\"dones\"]:\n",
    "            self.episode_rewards.append(self.locals[\"rewards\"][0])\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_deltas=random.sample(filtered_data, 200)\n",
    "\n",
    "env = LpEnv(training_deltas,data)\n",
    "\n",
    "\n",
    "# Check if the environment follows the Gym API\n",
    "#check_env(env, warn=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = make_vec_env(lambda: LpEnv(training_deltas,data), n_envs=1)\n",
    "\n",
    "# Choose the algorithm: PPO or SAC\n",
    "model = SAC('MlpPolicy', vec_env, verbose=1)\n",
    "reward_callback = RewardCallback()\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000, callback=reward_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"sac_custom_env\")\n",
    "\n",
    "# Load the model\n",
    "model = SAC.load(\"sac_custom_env\")\n",
    "\n",
    "# Plot the rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reward_callback.episode_rewards, label='Episode Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Over Training Duration')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
