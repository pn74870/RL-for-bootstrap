{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "class StandardDeviationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that uses the exact reward computation code provided.\n",
    "    Actions:\n",
    "        - Step in Delta1 space\n",
    "        - Step in Delta2 space\n",
    "    Observations:\n",
    "        - Delta1\n",
    "        - Delta2\n",
    "        - C1\n",
    "        - C2\n",
    "        - std_over_mean_sum (error metric)\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_delta1: float = 0.5,\n",
    "        max_delta1: float = 1.5,\n",
    "        min_delta2: float = 1.6,\n",
    "        max_delta2: float = 2.4,\n",
    "        delta_lr: float = 0.2,\n",
    "        max_episode_steps: int = 50,\n",
    "        z_samples: np.ndarray = np.linspace(0.3, 0.49, 20),\n",
    "    ):\n",
    "        super(StandardDeviationEnv, self).__init__()\n",
    "        self.data_type = np.float32\n",
    "\n",
    "        # Parameters\n",
    "        self.min_delta1 = min_delta1\n",
    "        self.max_delta1 = max_delta1\n",
    "        self.min_delta2 = min_delta2\n",
    "        self.max_delta2 = max_delta2\n",
    "        self.delta_lr = delta_lr  # Learning rate for delta updates\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "\n",
    "        # Initialize variables\n",
    "        self.reward = 0.0\n",
    "        self.std_over_mean_sum = None\n",
    "\n",
    "        # Initialize delta1 and delta2\n",
    "        self.delta1 = 1.56\n",
    "        self.delta2 = 2.7\n",
    "        # z_samples for main equations\n",
    "        self.z_samples = z_samples\n",
    "\n",
    "        # Action and observation spaces\n",
    "        # Actions: delta1_step, delta2_step\n",
    "        action_low = np.array([-1.0, -1.0], dtype=self.data_type)\n",
    "        action_high = np.array([1.0, 1.0], dtype=self.data_type)\n",
    "\n",
    "        # Observations: delta1, delta2, c1, c2, std_over_mean_sum\n",
    "        obs_low = np.array(\n",
    "            [self.min_delta1, self.min_delta2, -np.inf, -np.inf, 0.0],\n",
    "            dtype=self.data_type\n",
    "        )\n",
    "        obs_high = np.array(\n",
    "            [self.max_delta1, self.max_delta2, np.inf, np.inf, np.inf],\n",
    "            dtype=self.data_type\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Box(low=action_low, high=action_high, dtype=self.data_type)\n",
    "        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=self.data_type)\n",
    "\n",
    "        # Count steps\n",
    "        self.num_steps = 0\n",
    "\n",
    "        # Initialize state variables\n",
    "        self.reset()\n",
    "\n",
    "    # Functions provided by you\n",
    "\n",
    "    def ftoy(self, z, delta):\n",
    "        \"\"\"\n",
    "        Compute ftoy(z, delta) = Exp[-delta^2 * z] - Exp[-delta^2 * (1 - z)]\n",
    "        \"\"\"\n",
    "        return math.exp(-delta**2 * z) - math.exp(-delta**2 * (1 - z))\n",
    "\n",
    "    def main_equation(self, z, Delta1, Delta2):\n",
    "        \"\"\"\n",
    "        Compute the main equation coefficients B, C, D for given z, Delta1, Delta2.\n",
    "        Equation: B * C1 + C * C2 = D\n",
    "        where D = Exp[-4*(1 - z)] - Exp[-4*z] + 1/100*(Exp[-64*(1 - z)] - Exp[-64*z]) - 5*(-Exp[-1 + z] + Exp[-z])\n",
    "        \"\"\"\n",
    "        # Compute the constant terms\n",
    "        A = (math.exp(-4 * (1 - z)) - math.exp(-4 * z) +\n",
    "             (1/100) * (math.exp(-64 * (1 - z)) - math.exp(-64 * z)) -\n",
    "             5 * (-math.exp(-1 + z) + math.exp(-z)))\n",
    "\n",
    "        # Compute coefficients for C1 and C2\n",
    "        B = -math.exp(-((1 - z) * Delta1**2)) + math.exp(-z * Delta1**2)\n",
    "        C = -math.exp(-((1 - z) * Delta2**2)) + math.exp(-z * Delta2**2)\n",
    "\n",
    "        D = -A  # Rearranged to B*C1 + C*C2 = D\n",
    "        return (B, C, D)\n",
    "\n",
    "    def derivative_equations(self, Delta1, Delta2):\n",
    "        \"\"\"\n",
    "        Define the derivative equations explicitly as provided.\n",
    "        Each derivative equation is of the form:\n",
    "        B_deriv * C1 + C_deriv * C2 = D_deriv\n",
    "        \"\"\"\n",
    "        # Constants from the provided expressions\n",
    "        eq1 = (\n",
    "            2.0 * math.exp(-0.5 * Delta1**2) * Delta1**2,\n",
    "            2.0 * math.exp(-0.5 * Delta2**2) * Delta2**2,\n",
    "            7.147988863019252\n",
    "        )\n",
    "\n",
    "        eq2 = (\n",
    "            0.3333333333333333 * math.exp(-0.5 * Delta1**2) * Delta1**6,\n",
    "            0.3333333333333333 * math.exp(-0.5 * Delta2**2) * Delta2**6,\n",
    "            3.8980371419131926\n",
    "        )\n",
    "\n",
    "        eq3 = (\n",
    "            0.016666666666666667 * math.exp(-0.5 * Delta1**2) * Delta1**10,\n",
    "            0.016666666666666667 * math.exp(-0.5 * Delta2**2) * Delta2**10,\n",
    "            2.3602663911472502\n",
    "        )\n",
    "\n",
    "        eq4 = (\n",
    "            0.00039682539682539683 * math.exp(-0.5 * Delta1**2) * Delta1**14,\n",
    "            0.00039682539682539683 * math.exp(-0.5 * Delta2**2) * Delta2**14,\n",
    "            0.8810978138186971\n",
    "        )\n",
    "\n",
    "        eq5 = (\n",
    "            5.511463844797178e-6 * math.exp(-0.5 * Delta1**2) * Delta1**18,\n",
    "            5.511463844797178e-6 * math.exp(-0.5 * Delta2**2) * Delta2**18,\n",
    "            0.19556132338694376\n",
    "        )\n",
    "\n",
    "        return [eq1, eq2, eq3, eq4, eq5]\n",
    "\n",
    "    def get_all_equations(self, Delta1, Delta2):\n",
    "        \"\"\"\n",
    "        Combine main equations for all z_samples with derivative equations.\n",
    "        Returns a list of equations where each equation is a tuple (B, C, D).\n",
    "        Represented as B*C1 + C*C2 = D\n",
    "        \"\"\"\n",
    "        equations = []\n",
    "\n",
    "        # Main equations for each z in z_samples\n",
    "        for z in self.z_samples:\n",
    "            eq = self.main_equation(z, Delta1, Delta2)\n",
    "            equations.append(eq)\n",
    "\n",
    "        # Derivative equations\n",
    "        deriv_eqns = self.derivative_equations(Delta1, Delta2)\n",
    "        equations.extend(deriv_eqns)\n",
    "\n",
    "        return equations\n",
    "\n",
    "    def solve_subsets(self, equations):\n",
    "        \"\"\"\n",
    "        For all 2-element subsets of equations, solve for C1 and C2.\n",
    "        Collect all valid solutions.\n",
    "        Returns:\n",
    "            means: Tuple containing mean of C1 and mean of C2\n",
    "            stds: Tuple containing std of C1 and std of C2\n",
    "        \"\"\"\n",
    "        solutions = []\n",
    "        subsets = itertools.combinations(equations, 2)\n",
    "\n",
    "        for subset in subsets:\n",
    "            (B1, C1_coef, D1), (B2, C2_coef, D2) = subset\n",
    "            # Form the coefficient matrix and constant vector\n",
    "            A = np.array([[B1, C1_coef],\n",
    "                          [B2, C2_coef]], dtype=self.data_type)\n",
    "            D = np.array([D1, D2], dtype=self.data_type)\n",
    "            try:\n",
    "                sol = np.linalg.solve(A, D)\n",
    "                solutions.append(sol)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Singular or ill-conditioned matrix, skip this subset\n",
    "                continue\n",
    "\n",
    "        if len(solutions) == 0:\n",
    "            return None, None\n",
    "\n",
    "        solutions = np.array(solutions)\n",
    "        C1_mean = np.mean(solutions[:,0])\n",
    "        C1_std = np.std(solutions[:,0], ddof=1)\n",
    "        C2_mean = np.mean(solutions[:,1])\n",
    "        C2_std = np.std(solutions[:,1], ddof=1)\n",
    "\n",
    "        return (C1_mean, C2_mean), (C1_std, C2_std)\n",
    "\n",
    "    def get_reward(self, means, stds):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the mean and std of C1 and C2.\n",
    "        Reward = - (log(abs(std1 / mean1)) + log(abs(std2 / mean2)))\n",
    "        \"\"\"\n",
    "        C1_mean, C2_mean = means\n",
    "        C1_std, C2_std = stds\n",
    "\n",
    "        # Avoid division by zero by adding a small epsilon\n",
    "        epsilon = 1e-8\n",
    "        ratio1 = np.abs(C1_std / (C1_mean + epsilon))\n",
    "        ratio2 = np.abs(C2_std / (C2_mean + epsilon))\n",
    "\n",
    "        # Clip ratios to avoid log(0)\n",
    "        min_ratio = 1e-8\n",
    "        ratio1 = np.clip(ratio1, min_ratio, None)\n",
    "        ratio2 = np.clip(ratio2, min_ratio, None)\n",
    "\n",
    "        reward = - (np.log(ratio1) + np.log(ratio2))\n",
    "        return reward\n",
    "\n",
    "    def compute_reward(self, Delta1, Delta2):\n",
    "        \"\"\"\n",
    "        Compute the reward for given Delta1 and Delta2 based on the sampled z values.\n",
    "        \"\"\"\n",
    "        equations = self.get_all_equations(Delta1, Delta2)\n",
    "        means, stds = self.solve_subsets(equations)\n",
    "        if means is None or stds is None:\n",
    "            # Assign a very low reward if no solutions are found\n",
    "            self.cs = np.array([0.0, 0.0])\n",
    "            self.std_over_mean_sum = 1e6\n",
    "            return -1e6\n",
    "        reward = self.get_reward(means, stds)\n",
    "        self.cs = np.array(means)\n",
    "        # Compute std_over_mean_sum for observation\n",
    "        epsilon = 1e-8\n",
    "        cs_mean_safe = np.where(np.abs(self.cs) < epsilon, epsilon, np.abs(self.cs))\n",
    "        std_over_mean = np.array(stds) / cs_mean_safe\n",
    "        self.std_over_mean_sum = np.sum(std_over_mean)\n",
    "        return reward\n",
    "\n",
    "    # Rest of the environment methods\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate(\n",
    "            [np.array([self.delta1, self.delta2], dtype=self.data_type), self.cs, [self.reward]],\n",
    "            dtype=self.data_type\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.num_steps = 0\n",
    "\n",
    "        # Initialize delta1 and delta2 randomly within bounds\n",
    "        self.delta1 = np.random.uniform(self.min_delta1, self.max_delta1)\n",
    "        self.delta2 = np.random.uniform(self.min_delta2, self.max_delta2)\n",
    "\n",
    "        # Compute initial reward and observation\n",
    "        self.reward = self.compute_reward(self.delta1, self.delta2)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.num_steps += 1\n",
    "\n",
    "        # Update delta1 and delta2\n",
    "        delta1 = self.delta1 + self.delta_lr * action[0]\n",
    "        delta2 = self.delta2 + self.delta_lr * action[1]\n",
    "\n",
    "        # Check boundary conditions\n",
    "        is_ok = self.delta_bc(delta1, delta2)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if not is_ok:\n",
    "            # Penalize the agent for invalid deltas\n",
    "            self.reward = -10.0\n",
    "            done = True\n",
    "            observation = self._get_obs()\n",
    "            return observation, self.reward, done, {}\n",
    "\n",
    "        # Update deltas\n",
    "        self.delta1 = delta1\n",
    "        self.delta2 = delta2\n",
    "\n",
    "        # Compute the new reward\n",
    "        self.reward = self.compute_reward(self.delta1, self.delta2)\n",
    "\n",
    "        # Check if episode should be truncated due to max steps\n",
    "        if self.num_steps >= self.max_episode_steps:\n",
    "            done = True\n",
    "\n",
    "        # Check for NaNs in observation\n",
    "        observation = self._get_obs()\n",
    "        if np.isnan(observation).any():\n",
    "            self.reward = -10.0\n",
    "            done = True\n",
    "            return observation, self.reward, done, {}\n",
    "\n",
    "        return observation, self.reward, done, {}\n",
    "\n",
    "    def delta_bc(self, delta1, delta2):\n",
    "        # Check delta bounds\n",
    "        if delta1 < self.min_delta1 or delta1 > self.max_delta1:\n",
    "            return False\n",
    "        if delta2 < self.min_delta2 or delta2 > self.max_delta2:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Render the environment.\n",
    "        \"\"\"\n",
    "        print(f\"Step: {self.num_steps}\")\n",
    "        print(f\"Delta1: {self.delta1:.4f}, Delta2: {self.delta2:.4f}\")\n",
    "        print(f\"C1: {self.cs[0]:.4f}, C2: {self.cs[1]:.4f}\")\n",
    "        print(f\"Std over Mean Sum: {self.std_over_mean_sum:.4f}\")\n",
    "        print(f\"Reward: {self.reward:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the StandardDeviationEnv class is defined in the same script or imported appropriately\n",
    "\n",
    "# Initialize the environment\n",
    "env = StandardDeviationEnv()\n",
    "\n",
    "\n",
    "\n",
    "# Create the RL model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./ppo_standard_deviation_tensorboard/\")\n",
    "\n",
    "# Train the agent\n",
    "# Adjust the total_timesteps based on computational resources\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_standard_deviation\")\n",
    "\n",
    "# To load the model later:\n",
    "# model = PPO.load(\"ppo_standard_deviation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StandardDeviationEnv()\n",
    "\n",
    "# Number of evaluation episodes\n",
    "num_episodes = 100\n",
    "\n",
    "# Lists to store results\n",
    "delta1_list = []\n",
    "delta2_list = []\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    #print(observation)\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    Delta1, Delta2 = action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    delta1_list.append(Delta1)\n",
    "    delta2_list.append(Delta2)\n",
    "    reward_list.append(reward)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis\n",
    "delta1_array = np.array(delta1_list)\n",
    "delta2_array = np.array(delta2_list)\n",
    "reward_array = np.array(reward_list)\n",
    "\n",
    "# Find the best delta1 and delta2\n",
    "best_idx = np.argmax(reward_array)\n",
    "best_delta1 = delta1_array[best_idx]\n",
    "best_delta2 = delta2_array[best_idx]\n",
    "best_reward = reward_array[best_idx]\n",
    "\n",
    "print(f\"Best Delta1: {best_delta1}\")\n",
    "print(f\"Best Delta2: {best_delta2}\")\n",
    "print(f\"Best Reward: {best_reward}\")\n",
    "\n",
    "# Plotting the distribution of selected deltas\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(delta1_array, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Delta1')\n",
    "plt.xlabel('Delta1')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(delta2_array, bins=20, color='salmon', edgecolor='black')\n",
    "plt.title('Distribution of Delta2')\n",
    "plt.xlabel('Delta2')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot reward over episodes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_array, color='green')\n",
    "plt.title('Reward over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
